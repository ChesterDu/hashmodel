{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "hashmodel.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "2jA1-lKNkTaT"
      },
      "source": [
        "Necessary modules"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "WzWQiTKYiyJ6",
        "colab": {}
      },
      "source": [
        "import encoder\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import numpy as np\n",
        "import heapq\n",
        "import time\n",
        "import torch.optim as optim\n",
        "from nltk.translate.bleu_score import sentence_bleu, corpus_bleu, SmoothingFunction\n",
        "import pickle"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IJbpkeDvscfL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# get vocab\n",
        "model_dir = 'gpt_vocab'\n",
        "enc = encoder.get_encoder(model_dir)\n",
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "# define parameters\n",
        "class HParams():\n",
        "    n_vocab = 50257\n",
        "    n_embed = 512\n",
        "    start_token = 50256\n",
        "    batch_size = 25\n",
        "    device = device\n",
        " \n",
        "hparams = HParams()"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "kxENxXphk0G8",
        "colab": {}
      },
      "source": [
        "class Attn(nn.Module):\n",
        "    def __init__(self, input_dim = 512, hid_dim = 50, k = 5):\n",
        "        super(Attn,self).__init__()\n",
        "        self.input_dim = input_dim\n",
        "        self.hid_dim = hid_dim\n",
        "        # Number of triples selected\n",
        "        self.k = k\n",
        "        self.attn = nn.Sequential(nn.Linear(input_dim*2,hid_dim), nn.Linear(hid_dim,1))\n",
        "\n",
        "    def forward(self, h_k, h_c):\n",
        "        #h_k = [batch_size, k, hid_dim]\n",
        "        #h_c = [batch_size, hid_dim]\n",
        "\n",
        "        h_c =  h_c.unsqueeze(1) #[batch_size, 1, hid_dim]\n",
        "        h_c = torch.cat([h_c]*self.k,dim=1)    #[batch_size, k, hid_dim]\n",
        "        h_comb = torch.cat((h_k,h_c),dim=2)   #[batch_size, k, hid_dim*2]\n",
        "\n",
        "        attn_logits = self.attn(h_comb).squeeze(2)  #[batch_size,k]\n",
        "        attn_weight = F.softmax(attn_logits).unsqueeze(1)\n",
        "        h_k_comb = torch.bmm(attn_weight,h_k).squeeze(1)    #[batch_size,hid_dim]\n",
        "        return h_k_comb"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "k4A-X8jbkz-Q",
        "colab": {}
      },
      "source": [
        "class DecoderLSTM(nn.Module):\n",
        "    def __init__(self,embedding_size = 256, num_units = 512, vocab_size = 50257, dropout_p = 0.1, num_layers = 2):\n",
        "        super(DecoderLSTM, self).__init__()\n",
        "        self.embedding_size = embedding_size\n",
        "        self.num_units = num_units\n",
        "        self.vocab_size = vocab_size\n",
        "        self.dropout_p = dropout_p\n",
        "\n",
        "        self.embedding = nn.Embedding(vocab_size, embedding_size) # !\n",
        "\n",
        "        self.lstm = nn.LSTM(embedding_size,hidden_size=num_units,num_layers=num_layers, batch_first = True)\n",
        "\n",
        "        self.Linear = nn.Linear(num_units,vocab_size)\n",
        "\n",
        "        self.dropout = nn.Dropout(dropout_p)\n",
        "\n",
        "    def forward(self, input, hidden, cell):\n",
        "        #input = [batch_size]\n",
        "        #hidden = [batch_size, 2, hid_dim]\n",
        "        #cell = [batch_size, 2, hid_dim]\n",
        "\n",
        "        input = input.unsqueeze(1)  #[batch_size, 1]\n",
        "        # print(\"Input shape:\",input.shape)\n",
        "\n",
        "        embedding = self.dropout(self.embedding(input)) #[batch_size, 1, emb_dim]\n",
        "        # print(\"Embedding shape:\",embedding.shape)\n",
        "        # print(\"hidden shape\",hidden.shape)\n",
        "\n",
        "        output, (hidden, cell) = self.lstm(embedding, (hidden, cell)) \n",
        "\n",
        "        #output = [batch_size, 1, hid_dim]\n",
        "        #hidden = [2, batch_size hid_dim]\n",
        "        #cell  = [2, batch_size, hid_dim]\n",
        "\n",
        "        logits = self.Linear(output)  #[batch_size, vocab_size]\n",
        "\n",
        "        return logits, hidden, cell\n",
        "\n",
        "\n",
        "\n",
        "class Decoder(nn.Module):\n",
        "    def __init__(self, hparams, embedding_size = 256, num_units=512, vocab_size = 50257, dropout_p = 0.1, seq_len = 100, batch_size = 32, teacher_forcing_ratio = 0.5):\n",
        "        super(Decoder,self).__init__()\n",
        "\n",
        "        self.LSTM = DecoderLSTM(embedding_size, num_units,vocab_size,dropout_p)\n",
        "        self.start_token = hparams.start_token\n",
        "        self.batch_size = hparams.batch_size\n",
        "        self.teacher_forcing_ratio = teacher_forcing_ratio\n",
        "        self.hparams = hparams\n",
        "\n",
        "\n",
        "    def forward(self, trg, h_c, h_k):\n",
        "        #h_c = [batch_size, hid_dim]\n",
        "        #h_k = [batch_size, hid_dim]\n",
        "        #trg = [batch_size, seq_len]\n",
        "\n",
        "        input = trg[:,0]\n",
        "        hidden = torch.stack((h_c,h_k),dim=0)\n",
        "        cell = torch.zeros(2, self.batch_size,self.LSTM.num_units).to(self.hparams.device)\n",
        "        outputs = torch.zeros(self.batch_size, 1, self.LSTM.vocab_size).to(self.hparams.device)\n",
        "\n",
        "        for t in range(1, trg.shape[1]):\n",
        "            output, hidden, cell = self.LSTM.forward(input,hidden,cell)\n",
        "            # print(output.shape)\n",
        "\n",
        "            outputs = torch.cat([outputs,output],dim=1)\n",
        "            # outputs[:,t,:] = output\n",
        "\n",
        "            top1 = output.squeeze(1).argmax(1)\n",
        "            # print(\"top1_shape\",top1.shape)\n",
        "\n",
        "            replace = np.random.random() < self.teacher_forcing_ratio\n",
        "\n",
        "            input = trg[:,t] if replace else top1\n",
        "            # print(input.shape)\n",
        "\n",
        "        \n",
        "        return outputs[:,1:]\n",
        "\n",
        "\n",
        "    def decode(self, h_c, h_k, seq_len):\n",
        "      with torch.no_grad():\n",
        "        input = torch.LongTensor([self.hparams.start_token]*self.batch_size).to(self.hparams.device)\n",
        "        hidden = torch.stack((h_c,h_k),dim=0)\n",
        "        cell = torch.zeros(2,self.batch_size,self.LSTM.num_units).to(self.hparams.device)\n",
        "        tokens = None\n",
        "        # tokens = torch.LongTensor(np.zeros([self.batch_size,1]))\n",
        "        for t in range(0, seq_len-1):\n",
        "            output, hidden, cell = self.LSTM.forward(input,hidden,cell)\n",
        "            input = output.squeeze(1).argmax(1)\n",
        "            tokens = input.unsqueeze(1) if tokens == None else torch.cat([tokens,input.unsqueeze(1)],dim=1)\n",
        "        return tokens"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "70n-et9Yugh3",
        "colab": {}
      },
      "source": [
        "class Hashmodel(nn.Module):\n",
        "    def __init__(self,hparams,k):\n",
        "        super(Hashmodel,self).__init__()\n",
        "        self.embedding_model = nn.Sequential(\n",
        "                                            nn.Linear(768, 1024),\n",
        "                                            nn.Tanh(),\n",
        "                                            nn.Linear(1024, 512),\n",
        "                                            nn.Tanh(),\n",
        "                                            nn.Sigmoid(),\n",
        "                                            )\n",
        "        self.attn = Attn()\n",
        "        self.decoder = Decoder(hparams)\n",
        "        self.k = k\n",
        "        self.hparams = hparams\n",
        "\n",
        "    def find_k_tuples(self,tuples_emb, conv_emb, k):\n",
        "        \"\"\" \n",
        "        tuples is output tuple embedding from model [tuple_size, 512]\n",
        "        conv is output conversation embedding from model [batch_size,512]\n",
        "        k is the number of selected tuples \n",
        "        \"\"\"\n",
        "        # hash = [batch_size, 512]\n",
        "        # probability = [batch_size, 512]\n",
        "        topk_indices_lst = torch.LongTensor(np.zeros([conv_emb.shape[0],k])).to(self.hparams.device)\n",
        "        with torch.no_grad():\n",
        "            tuples_hash = torch.bernoulli(tuples_emb)\n",
        "            conv_hash = torch.bernoulli(conv_emb)\n",
        "            for i in range(conv_hash.shape[0]):\n",
        "                conv_hash_1 = conv_hash[i].detach()\n",
        "                hamming_table = torch.logical_xor(conv_hash_1, tuples_hash)\n",
        "                hamming_dist = torch.sum(hamming_table, 1)\n",
        "                _,topk_indices =  torch.topk(hamming_dist,k, largest = True)    ##[5]\n",
        "                topk_indices_lst[i] = topk_indices\n",
        "        \n",
        "            return topk_indices_lst, tuples_hash\n",
        "\n",
        "\n",
        "\n",
        "    def forward(self,conv,tuples,trg):\n",
        "        #conv = [batch_size,768]\n",
        "        #tuples = [tuples_size,768]\n",
        "        batch_size = self.hparams.batch_size\n",
        "        seq_len = trg.shape[1]\n",
        "\n",
        "        conv_emb = self.embedding_model(conv)   #[batch_size, 512]\n",
        "        tuple_emb = self.embedding_model(tuples) #[tuples_size, 512]\n",
        "        outputs = torch.zeros(1, batch_size , seq_len-1, self.hparams.n_vocab).to(self.hparams.device)\n",
        "        tokens = torch.zeros(1, batch_size, seq_len-1).to(self.hparams.device)\n",
        "        batch_pos = torch.zeros(1, batch_size).to(self.hparams.device)\n",
        "\n",
        "        ##Do five samples\n",
        "        for i in range(1):\n",
        "            topk_indices, tuple_hash = self.find_k_tuples(tuple_emb,conv_emb,self.k)    #[batch_size, 5]\n",
        "            topk_emb = tuple_emb[topk_indices]        #[batch_size, 5, 512]\n",
        "            topk_hash = tuple_hash[topk_indices]       #[batch_size, 5, 512]\n",
        "            temp_ones = torch.ones([batch_size, 5, 512]).to(self.hparams.device)\n",
        "            sum1 = torch.log(topk_emb) * topk_hash + torch.log(1 - topk_emb) * (1-topk_hash) #[batch_size, 5, 512]\n",
        "            sum2 = torch.sum(sum1,dim=2)\n",
        "            batch_pos[i] = torch.sum(sum2,dim=1) #[batch_size]\n",
        "            \n",
        "            # now go next level\n",
        "            after_attn = self.attn(topk_emb, conv_emb)\n",
        "            after_decode = self.decoder(trg,conv_emb,after_attn)\n",
        "            outputs[i] = after_decode\n",
        "            tokens[i] = self.decoder.decode(conv_emb, after_attn, seq_len) # [batch_size, seq_len]\n",
        "\n",
        "\n",
        "        return batch_pos, outputs, tokens\n",
        "        "
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "tags": [],
        "id": "g_NYlIqmscfa",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model = Hashmodel(hparams, 5).to(device)\n",
        "# init weights\n",
        "def init_weights(m):\n",
        "    for name, param in m.named_parameters():\n",
        "        nn.init.uniform_(param.data, -0.8, 0.8)\n",
        "\n",
        "model.apply(init_weights)\n",
        "\n",
        "# calculate the number of trainable parameters in the model\n",
        "def count_parameters(model):\n",
        "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
        "\n",
        "# optimizer\n",
        "optimizer = optim.Adam(model.parameters(),lr=1e-4)\n",
        "\n",
        "# index of <pad>\n",
        "PAD_ID = enc.encoder['<|endoftext|>']   \n",
        "# criterion\n",
        "# we ignore the loss whenever the target token is a padding token\n",
        "criterion = nn.CrossEntropyLoss(ignore_index = hparams.start_token)"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K6nxYF1cscfd",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def train(model, criterion, optimizer, tuples_emb, conv_emb, trg_tokens, hparams):\n",
        "#     model.to(device)\n",
        "    '''\n",
        "    tuples_emb is the embedding of all the tuples\n",
        "    conv_emb is the embedding of all the conversation \n",
        "    trg_tokens is the targer response\n",
        "    '''\n",
        "    \n",
        "    \n",
        "    model.train()\n",
        "    epoch_loss = 0\n",
        "    tuples_input = torch.FloatTensor(tuples_emb).to(hparams.device)\n",
        "    for iter in range(int(len(trg_tokens)/hparams.batch_size)):\n",
        "      if iter == 75:\n",
        "        continue\n",
        "      batched_data = get_batched_data(trg_tokens,conv_emb, hparams.batch_size, iter)\n",
        "      conv_input = torch.FloatTensor(batched_data['conv_emb']).to(hparams.device)\n",
        "      trg_input = torch.LongTensor(batched_data['trg_tokens']).to(hparams.device)\n",
        "\n",
        "      \n",
        "\n",
        "      optimizer.zero_grad()\n",
        "\n",
        "      batch_pos, outputs, tokens = model.forward(conv_input, tuples_input, trg_input)\n",
        "\n",
        "      #trg = [trg sent len, batch size]\n",
        "      #output = [trg sent len, batch size, output dim]\n",
        "\n",
        "      # output = output[1:].view(-1, output.shape[-1])\n",
        "      # trg = trg[1:].view(-1)\n",
        "\n",
        "      #output = [(trg sent len - 1) * batch size, output dim]\n",
        "      #trg = [(trg sent len - 1) * batch size]\n",
        "      # lm_loss = torch.zeros([5]).to(hparams.device)\n",
        "      # rl_loss = torch.zeros([5]).to(hparams.device)\n",
        "      for i in range(1):\n",
        "          output = outputs[i].view(-1, outputs.shape[-1])\n",
        "          trg = trg_input[:,1:].reshape(-1)\n",
        "          # output = outputs[i]\n",
        "          # trg = trg_input[:,1:]\n",
        "          lm_loss = criterion(output, trg) \n",
        "\n",
        "          token_text = batch_decode(tokens[i])\n",
        "          target_text = batch_decode(trg_input)\n",
        "          \n",
        "          temp = torch.Tensor(cal_bleu(token_text, target_text)).to(device)\n",
        "          rl_loss = -(torch.sum(batch_pos[i] * temp ).to(device) / len(token_text))\n",
        "          \n",
        "      # print(lm_loss.dtype)\n",
        "      # print(rl_loss)\n",
        "      # loss = (torch.sum(lm_loss) + torch.sum(rl_loss)) / 5\n",
        "      loss = lm_loss + rl_loss\n",
        "\n",
        "      loss.backward()\n",
        "\n",
        "      torch.nn.utils.clip_grad_norm_(model.parameters(), 1)\n",
        "\n",
        "      optimizer.step()\n",
        "\n",
        "      del(conv_input)\n",
        "      del(trg_input)\n",
        "\n",
        "      print(\"iter %d loss: %f\"%(iter,loss.item()))\n",
        "      torch.cuda.empty_cache()\n",
        "\n",
        "    epoch_loss += loss.item()\n",
        "\n",
        "    return epoch_loss  "
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "tags": [],
        "id": "O6oS5T3lscff",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def batch_decode(tokens):\n",
        "    \n",
        "    tokens = tokens.data.cpu().numpy() # [batch_size, seq_len]\n",
        "    result = []\n",
        "    for token in tokens:\n",
        "        result.append(enc.decode(token).replace('<|endoftext|>',''))\n",
        "    return result"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "tags": [],
        "id": "0h8RC9Etscfi",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def cal_bleu(token_text, target_text):\n",
        "    smooth = SmoothingFunction()\n",
        "    score = []\n",
        "    for i in range(len(token_text)):\n",
        "        reference = [target_text[i].strip().split()]\n",
        "        candidate = token_text[i].strip().split()\n",
        "        bleu = sentence_bleu(reference, candidate,smoothing_function=smooth.method1)\n",
        "        bleu -= 0.2\n",
        "        score.append(bleu)\n",
        "    return score "
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jq5pmmGW-p8S",
        "colab_type": "text"
      },
      "source": [
        "##Data Loader"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hhlzj1qhv3p1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def EmbLoader(kg_path, qs_path):\n",
        "    responds = []\n",
        "    questions = []\n",
        "    r_flag = False\n",
        "    list_file = open(qs_path,'rb')\n",
        "    embeddings = pickle.load(list_file)\n",
        "    list_file.close()\n",
        "    for line in embeddings:\n",
        "        if r_flag == False:\n",
        "            questions.append(line)\n",
        "            r_flag = True\n",
        "        else:\n",
        "            responds.append(line)\n",
        "            r_flag = False\n",
        "\n",
        "    list_file = open(kg_path,'rb')\n",
        "    tuples = pickle.load(list_file)\n",
        "    list_file.close()\n",
        "    return np.array(questions), np.array(tuples)\n",
        "    \n",
        "def TextLoader(trg_path,size):\n",
        "  with open(trg_path,'r') as fin:\n",
        "    r_flag = False\n",
        "    response_lst = []\n",
        "    count = 0\n",
        "    for line in fin:\n",
        "      if not r_flag:\n",
        "        r_flag = True\n",
        "        continue\n",
        "      else:\n",
        "        response_lst.append(line.strip())\n",
        "        count += 1\n",
        "        if count >= size:\n",
        "          break\n",
        "        r_flag = False\n",
        "    response_token = [enc.encode(response) for response in response_lst]\n",
        "    return response_token\n",
        "\n",
        "def get_batched_data(tokens,conv_emb,batch_size,iter_num):\n",
        "  assert(len(tokens) == len(conv_emb))\n",
        "  batched_data = {}\n",
        "  st = batch_size*iter_num\n",
        "  ed = batch_size*(iter_num + 1)\n",
        "  if ed >= len(tokens):\n",
        "    ed = len(tokens)\n",
        "    \n",
        "  batched_tokens = tokens[st:ed]\n",
        "  max_len = max([len(text) for text in batched_tokens]) + 2\n",
        "  batched_pad_tokens = pad_text(batched_tokens,max_len)\n",
        "  batched_data['trg_tokens'] = batched_pad_tokens\n",
        "  batched_data['conv_emb'] = conv_emb[st:ed]\n",
        "  return batched_data\n",
        "\n",
        "def pad_text(text,max_len):\n",
        "  pad_texts = [[PAD_ID] + line + [PAD_ID]*(max_len - len(line)) for line in text]\n",
        "  return np.array(pad_texts)\n"
      ],
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "tags": [],
        "id": "0n6vKYQsscfu",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "ae99ad90-a8f4-4998-a02b-5f8cd9f3ead8"
      },
      "source": [
        "questions_emb, tuples_emb = EmbLoader('tuples.pickle', 'dialogue_embeddings.pickle')\n",
        "tokens = TextLoader(\"dialogue.txt\",5000)\n",
        "train(model, criterion, optimizer, tuples_emb, questions_emb, tokens, hparams)\n"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:19: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "iter 0 loss: -289.681976\n",
            "iter 1 loss: -289.304779\n",
            "iter 2 loss: -288.792542\n",
            "iter 3 loss: -289.701904\n",
            "iter 4 loss: -290.020966\n",
            "iter 5 loss: -288.358795\n",
            "iter 6 loss: -288.484650\n",
            "iter 7 loss: -287.538391\n",
            "iter 8 loss: -287.863190\n",
            "iter 9 loss: -287.650574\n",
            "iter 10 loss: -288.951904\n",
            "iter 11 loss: -286.734344\n",
            "iter 12 loss: -289.588959\n",
            "iter 13 loss: -287.166016\n",
            "iter 14 loss: -288.304291\n",
            "iter 15 loss: -285.470520\n",
            "iter 16 loss: -288.210571\n",
            "iter 17 loss: -286.598755\n",
            "iter 18 loss: -288.036438\n",
            "iter 19 loss: -287.838379\n",
            "iter 20 loss: -287.253845\n",
            "iter 21 loss: -287.223053\n",
            "iter 22 loss: -288.381622\n",
            "iter 23 loss: -288.208893\n",
            "iter 24 loss: -287.111786\n",
            "iter 25 loss: -287.137085\n",
            "iter 26 loss: -288.709259\n",
            "iter 27 loss: -286.741669\n",
            "iter 28 loss: -288.962646\n",
            "iter 29 loss: -287.740601\n",
            "iter 30 loss: -288.674866\n",
            "iter 31 loss: -288.544434\n",
            "iter 32 loss: -288.384216\n",
            "iter 33 loss: -289.103394\n",
            "iter 34 loss: -288.286926\n",
            "iter 35 loss: -289.148132\n",
            "iter 36 loss: -288.297394\n",
            "iter 37 loss: -287.799255\n",
            "iter 38 loss: -288.518524\n",
            "iter 39 loss: -288.372711\n",
            "iter 40 loss: -288.948578\n",
            "iter 41 loss: -289.325409\n",
            "iter 42 loss: -287.962219\n",
            "iter 43 loss: -287.566071\n",
            "iter 44 loss: -287.612457\n",
            "iter 45 loss: -288.952698\n",
            "iter 46 loss: -288.136169\n",
            "iter 47 loss: -288.863159\n",
            "iter 48 loss: -289.432434\n",
            "iter 49 loss: -289.297455\n",
            "iter 50 loss: -288.814240\n",
            "iter 51 loss: -287.254150\n",
            "iter 52 loss: -288.883209\n",
            "iter 53 loss: -287.317505\n",
            "iter 54 loss: -287.552246\n",
            "iter 55 loss: -288.847839\n",
            "iter 56 loss: -288.162689\n",
            "iter 57 loss: -288.013153\n",
            "iter 58 loss: -288.475403\n",
            "iter 59 loss: -289.475433\n",
            "iter 60 loss: -287.897064\n",
            "iter 61 loss: -289.843262\n",
            "iter 62 loss: -288.343811\n",
            "iter 63 loss: -289.028076\n",
            "iter 64 loss: -287.852966\n",
            "iter 65 loss: -291.477722\n",
            "iter 66 loss: -287.770294\n",
            "iter 67 loss: -288.046295\n",
            "iter 68 loss: -288.889832\n",
            "iter 69 loss: -287.335327\n",
            "iter 70 loss: -285.886627\n",
            "iter 71 loss: -285.965546\n",
            "iter 72 loss: -289.467529\n",
            "iter 73 loss: -287.424225\n",
            "iter 74 loss: -288.881531\n",
            "iter 76 loss: -287.752838\n",
            "iter 77 loss: -287.618591\n",
            "iter 78 loss: -287.971497\n",
            "iter 79 loss: -286.713287\n",
            "iter 80 loss: -289.212738\n",
            "iter 81 loss: -287.984314\n",
            "iter 82 loss: -288.388336\n",
            "iter 83 loss: -289.712585\n",
            "iter 84 loss: -289.540222\n",
            "iter 85 loss: -287.282990\n",
            "iter 86 loss: -288.400269\n",
            "iter 87 loss: -289.163940\n",
            "iter 88 loss: -290.352661\n",
            "iter 89 loss: -286.980042\n",
            "iter 90 loss: -289.641418\n",
            "iter 91 loss: -287.515656\n",
            "iter 92 loss: -288.266571\n",
            "iter 93 loss: -287.306458\n",
            "iter 94 loss: -287.993988\n",
            "iter 95 loss: -286.859985\n",
            "iter 96 loss: -286.999695\n",
            "iter 97 loss: -287.982391\n",
            "iter 98 loss: -289.652954\n",
            "iter 99 loss: -288.879639\n",
            "iter 100 loss: -287.076477\n",
            "iter 101 loss: -288.470032\n",
            "iter 102 loss: -288.427185\n",
            "iter 103 loss: -289.021454\n",
            "iter 104 loss: -288.980682\n",
            "iter 105 loss: -287.552521\n",
            "iter 106 loss: -288.606140\n",
            "iter 107 loss: -288.152100\n",
            "iter 108 loss: -288.626190\n",
            "iter 109 loss: -288.947571\n",
            "iter 110 loss: -286.987549\n",
            "iter 111 loss: -287.887329\n",
            "iter 112 loss: -287.177673\n",
            "iter 113 loss: -289.842468\n",
            "iter 114 loss: -288.128571\n",
            "iter 115 loss: -289.318604\n",
            "iter 116 loss: -287.837341\n",
            "iter 117 loss: -288.907135\n",
            "iter 118 loss: -290.683807\n",
            "iter 119 loss: -287.571625\n",
            "iter 120 loss: -287.629883\n",
            "iter 121 loss: -287.853821\n",
            "iter 122 loss: -289.567505\n",
            "iter 123 loss: -288.460938\n",
            "iter 124 loss: -289.182800\n",
            "iter 125 loss: -286.639008\n",
            "iter 126 loss: -289.580353\n",
            "iter 127 loss: -287.722321\n",
            "iter 128 loss: -288.040009\n",
            "iter 129 loss: -289.152649\n",
            "iter 130 loss: -288.126129\n",
            "iter 131 loss: -288.414520\n",
            "iter 132 loss: -288.044678\n",
            "iter 133 loss: -287.329865\n",
            "iter 134 loss: -288.288879\n",
            "iter 135 loss: -289.482635\n",
            "iter 136 loss: -288.641205\n",
            "iter 137 loss: -288.411194\n",
            "iter 138 loss: -289.273590\n",
            "iter 139 loss: -289.945312\n",
            "iter 140 loss: -288.532104\n",
            "iter 141 loss: -288.782166\n",
            "iter 142 loss: -288.069702\n",
            "iter 143 loss: -286.978180\n",
            "iter 144 loss: -289.099243\n",
            "iter 145 loss: -288.792297\n",
            "iter 146 loss: -288.305573\n",
            "iter 147 loss: -287.880157\n",
            "iter 148 loss: -287.319336\n",
            "iter 149 loss: -289.452850\n",
            "iter 150 loss: -286.947113\n",
            "iter 151 loss: -287.838715\n",
            "iter 152 loss: -287.431000\n",
            "iter 153 loss: -288.471436\n",
            "iter 154 loss: -287.848145\n",
            "iter 155 loss: -288.772278\n",
            "iter 156 loss: -287.094574\n",
            "iter 157 loss: -288.216064\n",
            "iter 158 loss: -285.821747\n",
            "iter 159 loss: -288.882935\n",
            "iter 160 loss: -289.377014\n",
            "iter 161 loss: -286.478485\n",
            "iter 162 loss: -287.799438\n",
            "iter 163 loss: -288.848297\n",
            "iter 164 loss: -288.276184\n",
            "iter 165 loss: -289.246368\n",
            "iter 166 loss: -289.316742\n",
            "iter 167 loss: -288.849548\n",
            "iter 168 loss: -288.970612\n",
            "iter 169 loss: -287.260223\n",
            "iter 170 loss: -289.207336\n",
            "iter 171 loss: -288.211639\n",
            "iter 172 loss: -287.383453\n",
            "iter 173 loss: -287.687775\n",
            "iter 174 loss: -289.362091\n",
            "iter 175 loss: -288.957275\n",
            "iter 176 loss: -287.357208\n",
            "iter 177 loss: -289.658600\n",
            "iter 178 loss: -289.448547\n",
            "iter 179 loss: -288.073547\n",
            "iter 180 loss: -290.984375\n",
            "iter 181 loss: -288.766602\n",
            "iter 182 loss: -288.838196\n",
            "iter 183 loss: -288.488922\n",
            "iter 184 loss: -288.795532\n",
            "iter 185 loss: -288.673553\n",
            "iter 186 loss: -287.259460\n",
            "iter 187 loss: -287.332977\n",
            "iter 188 loss: -287.893646\n",
            "iter 189 loss: -286.893951\n",
            "iter 190 loss: -286.804657\n",
            "iter 191 loss: -288.959320\n",
            "iter 192 loss: -287.399323\n",
            "iter 193 loss: -288.290741\n",
            "iter 194 loss: -288.453278\n",
            "iter 195 loss: -289.047607\n",
            "iter 196 loss: -286.610474\n",
            "iter 197 loss: -287.446472\n",
            "iter 198 loss: -288.313324\n",
            "iter 199 loss: -288.368561\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "-288.3685607910156"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ScF_QLBsscfx",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "tokens = get_batched_data(tokens,questions_emb, hparams.batch_size, 75)['trg_tokens']\n",
        "for line in tokens:\n",
        "  for id in line:\n",
        "    if id > 50256 or id < 0:\n",
        "      print(\"Error\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mTXBQ85Tscfz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from torchviz import make_dot\n",
        "tuples_input = torch.randn(500, 768).to(device)\n",
        "conv_input = torch.randn(32, 768).to(device)\n",
        "trg_input = torch.LongTensor(np.random.randint(0,50256, [32,20])).to(device)\n",
        "\n",
        "vis_graph = make_dot(model(conv_input,tuples_input,trg_input), params=dict(model.named_parameters()))\n",
        "vis_graph.view()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RtXg1FCdilRH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!nvidia-smi"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}